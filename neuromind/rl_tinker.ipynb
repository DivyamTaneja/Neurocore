{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from controller import (\n",
    "    ControllerState,\n",
    "    get_action_space,\n",
    "    execute_action,\n",
    "    get_observation,\n",
    "    compute_reward\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "def load_random_image_to_spikemem(\n",
    "    spike_mem,\n",
    "    dataset,\n",
    "    ttfs_encoder,\n",
    "    Tmax,\n",
    "    input_layer=0\n",
    "):\n",
    "    # sample random image\n",
    "    idx = random.randint(0, len(dataset) - 1)\n",
    "    img, label = dataset[idx]\n",
    "\n",
    "    img_tensor = img.unsqueeze(0)   # [1, C, H, W]\n",
    "    C, H, W = img.shape\n",
    "\n",
    "    # TTFS encode\n",
    "    spike_seq = ttfs_encoder(img_tensor)  # [1, T, C, H, W]\n",
    "    spike_seq = spike_seq.squeeze(0).cpu().numpy()  # [T, C, H, W]\n",
    "\n",
    "    # reset input layer\n",
    "    spike_mem.reset_layer(input_layer)\n",
    "\n",
    "    # load spikes\n",
    "    for t in range(Tmax):\n",
    "        cur = spike_seq[t]\n",
    "        for ch in range(C):\n",
    "            for r in range(H):\n",
    "                for c in range(W):\n",
    "                    if cur[ch, r, c] != 0.0:\n",
    "                        spike_mem.put_spike(\n",
    "                            layer=input_layer,\n",
    "                            t=t,\n",
    "                            ch=ch,\n",
    "                            row=r,\n",
    "                            col=c\n",
    "                        )\n",
    "\n",
    "    return label   # useful for logging / debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeSchedulingEnv(gym.Env):\n",
    "    def __init__(self, encoder, processor, spike_mem, neuron_mem, dataset, output_layer):\n",
    "        super().__init__()\n",
    "        self.processor = processor\n",
    "        self.spike_mem = spike_mem\n",
    "        self.neuron_mem = neuron_mem\n",
    "        self.output_layer = output_layer\n",
    "        self.dataset = dataset\n",
    "        self.ttfs = encoder\n",
    "        self.Tmax = 8\n",
    "\n",
    "        self.state = ControllerState(processor, 8)\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=np.inf, shape=(5,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.last_action = None\n",
    "        # action_space is dynamic â†’ we handle indexing manually\n",
    "\n",
    "    def reset(self):\n",
    "        # reset internal state\n",
    "        self.state.reset()\n",
    "        self.neuron_mem.reset_all()\n",
    "        self.spike_mem.reset_all()\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "\n",
    "        # ðŸ”¹ load random CIFAR image into SpikeMemory\n",
    "        self.current_label = load_random_image_to_spikemem(\n",
    "            spike_mem=self.spike_mem,\n",
    "            dataset=self.dataset,          # stored in env\n",
    "            ttfs_encoder=self.ttfs,         # stored in env\n",
    "            Tmax=self.Tmax,\n",
    "            input_layer=0\n",
    "        )\n",
    "\n",
    "        # compute initial legal actions & observation\n",
    "        actions = get_action_space(\n",
    "            self.state,\n",
    "            self.processor\n",
    "        )\n",
    "\n",
    "        obs = get_observation(\n",
    "            self.spike_mem,\n",
    "            self.neuron_mem,\n",
    "            self.state,\n",
    "            actions\n",
    "        )\n",
    "\n",
    "        self.last_action = None\n",
    "\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action_idx):\n",
    "        if self.done:\n",
    "            raise RuntimeError(\"Episode already done\")\n",
    "\n",
    "        actions = get_action_space(self.state, self.processor)\n",
    "\n",
    "        if not actions:\n",
    "            # deadlock\n",
    "            reward = -100.0\n",
    "            self.done = True\n",
    "            obs = get_observation(self.spike_mem, self.neuron_mem, self.state, actions)\n",
    "            return obs, reward, True, {\"deadlock\": True}\n",
    "\n",
    "        prev_spikes = self.spike_mem.count_total_spikes()\n",
    "        prev_neurons = self.neuron_mem.total_active()\n",
    "\n",
    "        # Execute chosen action\n",
    "        action = actions[action_idx]\n",
    "\n",
    "        action_repeated = (\n",
    "            self.last_action is not None\n",
    "            and action == self.last_action\n",
    "        )\n",
    "        self.last_action = action\n",
    "\n",
    "        execute_action(action, self.state, self.processor,\n",
    "                    self.spike_mem, self.neuron_mem)\n",
    "\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Check termination\n",
    "        done = self.spike_mem.has_output_spike(self.output_layer)\n",
    "        deadlock = False\n",
    "\n",
    "        reward = compute_reward(\n",
    "            prev_spikes,\n",
    "            prev_neurons,\n",
    "            self.spike_mem,\n",
    "            self.neuron_mem,\n",
    "            done,\n",
    "            deadlock,\n",
    "            action_repeated=action_repeated\n",
    "        )\n",
    "\n",
    "        obs = get_observation(self.spike_mem, self.neuron_mem, self.state, actions)\n",
    "        self.done = done\n",
    "\n",
    "        return obs, reward, done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ACTIONS = 256      # upper bound on possible actions\n",
    "STATE_DIM = 5          # [spikes, neurons, n_actions, min_t, max_t]\n",
    "\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "LR = 3e-4\n",
    "ENTROPY_COEF = 0.01\n",
    "VALUE_COEF = 0.5\n",
    "\n",
    "Tmax = 8\n",
    "NUM_LAYERS = 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class PPOPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Used during rollout\n",
    "    # -----------------------------\n",
    "    def act(self, obs, legal_actions=None):\n",
    "        logits = self.actor(obs)\n",
    "\n",
    "        if legal_actions is not None:\n",
    "            mask = torch.full_like(logits, -1e9)\n",
    "            mask[legal_actions] = 0\n",
    "            logits = logits + mask\n",
    "\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "\n",
    "        log_prob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        value = self.critic(obs)\n",
    "\n",
    "        return action.item(), log_prob, value, entropy\n",
    "\n",
    "    # -----------------------------\n",
    "    # REQUIRED for PPO UPDATE\n",
    "    # -----------------------------\n",
    "    def evaluate(self, states, actions):\n",
    "        \"\"\"\n",
    "        states:  [B, obs_dim]\n",
    "        actions: [B] (LongTensor)\n",
    "        \"\"\"\n",
    "        logits = self.actor(states)\n",
    "        dist = Categorical(logits=logits)\n",
    "\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        values = self.critic(states).squeeze(-1)\n",
    "\n",
    "        return log_probs, values, entropy\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        state: [state_dim]\n",
    "        \"\"\"\n",
    "        logits = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(model, state, action_mask):\n",
    "    \"\"\"\n",
    "    state: torch.Tensor [state_dim]\n",
    "    action_mask: torch.Tensor [MAX_ACTIONS] (1 = valid, 0 = invalid)\n",
    "    \"\"\"\n",
    "    logits, value = model(state)\n",
    "\n",
    "    # mask invalid actions\n",
    "    masked_logits = logits.clone()\n",
    "    masked_logits[action_mask == 0] = -1e9\n",
    "\n",
    "    dist = torch.distributions.Categorical(logits=masked_logits)\n",
    "    action = dist.sample()\n",
    "\n",
    "    log_prob = dist.log_prob(action)\n",
    "    entropy = dist.entropy()\n",
    "\n",
    "    return action.item(), log_prob, value, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.clear()\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        self.entropies = []\n",
    "\n",
    "    def compute_returns(self):\n",
    "        \"\"\"\n",
    "        Standard discounted returns (NO GAE yet)\n",
    "        \"\"\"\n",
    "        returns = []\n",
    "        G = 0.0\n",
    "\n",
    "        for reward, done in zip(\n",
    "            reversed(self.rewards),\n",
    "            reversed(self.dones)\n",
    "        ):\n",
    "            if done:\n",
    "                G = 0.0\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "\n",
    "        return torch.tensor(returns, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, gamma=0.99, lam=0.95):\n",
    "    advantages = []\n",
    "    gae = 0.0\n",
    "\n",
    "    values = values + [0.0]\n",
    "\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t+1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "\n",
    "    returns = [adv + val for adv, val in zip(advantages, values[:-1])]\n",
    "    return advantages, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(model, optimizer, buffer, epochs=4):\n",
    "    states = torch.stack(buffer.states)\n",
    "    actions = torch.tensor(buffer.actions)\n",
    "    old_log_probs = torch.stack(buffer.log_probs)\n",
    "    returns = buffer.compute_returns()\n",
    "    advantages = returns - torch.stack(buffer.values)\n",
    "\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        log_probs, values, entropy = model.evaluate(states, actions)\n",
    "\n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advantages\n",
    "\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = F.mse_loss(values.squeeze(), returns)\n",
    "\n",
    "        loss = (\n",
    "            policy_loss\n",
    "            + VALUE_COEF * value_loss\n",
    "            - ENTROPY_COEF * entropy.mean()\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spike_memory import SpikeMemory\n",
    "from neuron_memory import NeuronMemory\n",
    "from spike_processor import SpikeProcessor\n",
    "from snn_model import SCNN_CIFAR10_TTFS, TTFS_Encoder\n",
    "\n",
    "encoder = TTFS_Encoder(T=Tmax)\n",
    "\n",
    "spike_mem = SpikeMemory(num_layers=NUM_LAYERS, Tmax=Tmax)\n",
    "neuron_mem = NeuronMemory()\n",
    "\n",
    "model = SCNN_CIFAR10_TTFS()\n",
    "model.load_state_dict(\n",
    "    torch.load(\"ttfs_based_scnn_model_weights.pth\", map_location=\"cpu\"),\n",
    "    strict=False\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "processor = SpikeProcessor(\n",
    "    model,\n",
    "    neuron_mem,\n",
    "    spike_mem\n",
    ")\n",
    "\n",
    "output_layer = max(processor.shapes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/divyam/anaconda3/envs/snn/lib/python3.12/site-packages/torch/autograd/graph.py:841: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 | Reward=359.00 | \n",
      "Ep 1 | Reward=848.50 | \n",
      "Ep 2 | Reward=481.00 | \n",
      "Ep 3 | Reward=344.00 | \n",
      "Ep 4 | Reward=778.00 | \n",
      "Ep 5 | Reward=166.00 | \n",
      "Ep 6 | Reward=477.00 | \n",
      "Ep 7 | Reward=473.50 | \n",
      "Ep 8 | Reward=505.00 | \n",
      "Ep 9 | Reward=182.00 | \n",
      "Ep 10 | Reward=264.00 | \n",
      "Ep 11 | Reward=195.00 | \n",
      "Ep 12 | Reward=363.00 | \n",
      "Ep 13 | Reward=600.00 | \n",
      "Ep 14 | Reward=587.00 | \n",
      "Ep 15 | Reward=173.00 | \n",
      "Ep 16 | Reward=591.00 | \n",
      "Ep 17 | Reward=643.00 | \n",
      "Ep 18 | Reward=284.00 | \n",
      "Ep 19 | Reward=568.00 | \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     28\u001b[39m action_mask[:\u001b[38;5;28mlen\u001b[39m(actions)] = \u001b[32m1.0\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 2. Prepare state tensor (NO grad here)\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m state = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# 3. Sample action from policy\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------\u001b[39;00m\n\u001b[32m     38\u001b[39m action, log_prob, value, entropy = select_action(\n\u001b[32m     39\u001b[39m     model, state, action_mask\n\u001b[32m     40\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "env = SpikeSchedulingEnv(encoder, processor, spike_mem, neuron_mem, train_dataset, output_layer)\n",
    "model = PPOPolicy(STATE_DIM, MAX_ACTIONS)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "buffer = RolloutBuffer()\n",
    "\n",
    "for episode in range(1000):\n",
    "    obs = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # -------------------------------------------------\n",
    "        # 1. Query legal actions from environment\n",
    "        # -------------------------------------------------\n",
    "        actions = get_action_space(env.state, env.processor)\n",
    "\n",
    "        # print(\"Legal actions:\")\n",
    "        # for i, (L, R) in enumerate(actions):\n",
    "        #     print(\n",
    "        #         f\"[{i}] Layer={L}, Row={R}, \"\n",
    "        #         f\"t={env.state.exec_t[(L, R)]}\"\n",
    "        #     )\n",
    "\n",
    "        action_mask = torch.zeros(MAX_ACTIONS, dtype=torch.float32)\n",
    "        action_mask[:len(actions)] = 1.0\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 2. Prepare state tensor (NO grad here)\n",
    "        # -------------------------------------------------\n",
    "        state = torch.tensor(obs, dtype=torch.float32)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 3. Sample action from policy\n",
    "        # -------------------------------------------------\n",
    "        action, log_prob, value, entropy = select_action(\n",
    "            model, state, action_mask\n",
    "        )\n",
    "\n",
    "        # print(action)\n",
    "        # idx = int(input(\"Choose action index: \"))\n",
    "        \n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 4. Step environment\n",
    "        # -------------------------------------------------\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 5. Store rollout (ðŸ”¥ DETACH EVERYTHING ðŸ”¥)\n",
    "        # -------------------------------------------------\n",
    "        buffer.states.append(state.detach())              # state is already no-grad, but safe\n",
    "        buffer.actions.append(int(action))                # store as int\n",
    "        buffer.log_probs.append(log_prob.detach())\n",
    "        buffer.values.append(value.detach().squeeze())\n",
    "        buffer.entropies.append(entropy.detach())\n",
    "\n",
    "        buffer.rewards.append(float(reward))\n",
    "        buffer.dones.append(bool(done))\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 6. Move to next state\n",
    "        # -------------------------------------------------\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "    ppo_update(model, optimizer, buffer)\n",
    "    buffer.clear()\n",
    "\n",
    "    # print(f\"Episode {episode} finished\")\n",
    "    print(\n",
    "    f\"Ep {episode} | \"\n",
    "    f\"Reward={episode_reward:.2f} | \"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
